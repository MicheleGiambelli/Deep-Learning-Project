{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuqzLq428Bm9ILa9BDhudw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicheleGiambelli/Deep-Learning-Project/blob/tommy/Backup_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm2IfqYijLxG"
      },
      "outputs": [],
      "source": [
        "mean = [0.5437, 0.4453, 0.3496]\n",
        "std = [0.2687, 0.2710, 0.2731]\n",
        "\n",
        "dataset, train_loader, val_loader, test_loader = data_loader(dataset_path = dataset_path,\n",
        "                                                             batch_size = 64,\n",
        "                                                             height = 224, width = 224,\n",
        "                                                             mean = mean, std = std)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels dictionary\n",
        "class_to_idx = dataset.class_to_idx\n",
        "print(class_to_idx)"
      ],
      "metadata": {
        "id": "S393QkMujU4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "    def __init__(self, input_channels, c1, c2, c3, c4, **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_channels (int): Numero di canali in ingresso.\n",
        "            c1 (int): Numero di canali di output per il percorso 1 (conv 1x1).\n",
        "            c2 (tuple): (c2_reduce, c2_out), riduzione con 1x1 -> convoluzione 3x3.\n",
        "            c3 (tuple): (c3_reduce, c3_out), riduzione con 1x1 -> convoluzione 5x5.\n",
        "            c4 (int): Numero di canali di output per il percorso 4 (pooling + conv 1x1).\n",
        "        \"\"\"\n",
        "        super(Inception, self).__init__(**kwargs) #super --> costruttore di nn.Module fondamentale in Pytorch per inizializzare la classe correttamente\n",
        "\n",
        "        # Path 1: Single 1x1 Convolution\n",
        "        self.path1 = nn.Conv2d(input_channels, c1, kernel_size=1)\n",
        "\n",
        "        # Path 2: 1x1 -> 3x3 Convolution\n",
        "        self.path2_1 = nn.Conv2d(input_channels, c2[0], kernel_size=1)\n",
        "        self.path2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
        "\n",
        "        # Path 3: 1x1 -> 5x5 Convolution\n",
        "        self.path3_1 = nn.Conv2d(input_channels, c3[0], kernel_size=1)\n",
        "        self.path3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
        "\n",
        "        # Path 4: Max Pooling -> 1x1 Convolution\n",
        "        self.path4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.path4_2 = nn.Conv2d(input_channels, c4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        p1 = self.path1(x)\n",
        "        p2 = self.path2_2(self.path2_1(x))\n",
        "        p3 = self.path3_2(self.path3_1(x))\n",
        "        p4 = self.path4_2(self.path4_1(x))\n",
        "\n",
        "        # Concatenate along the channel dimension\n",
        "        return torch.cat((p1, p2, p3, p4), dim=1)"
      ],
      "metadata": {
        "id": "oGEVOxp7kGIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blocco 1\n",
        "b1 = nn.Sequential(\n",
        "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        ")\n",
        "\n",
        "# Blocco 2\n",
        "b2 = nn.Sequential(\n",
        "    nn.Conv2d(64, 64, kernel_size=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        ")\n",
        "\n",
        "# Blocco 3\n",
        "b3 = nn.Sequential(\n",
        "    Inception(192, 64, (96, 128), (16, 32), 32),\n",
        "    Inception(256, 128, (128, 192), (32, 96), 64),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        ")\n",
        "\n",
        "# Blocco 4\n",
        "b4 = nn.Sequential(\n",
        "    Inception(480, 192, (96, 208), (16, 48), 64),\n",
        "    Inception(512, 160, (112, 224), (24, 64), 64),\n",
        "    Inception(512, 128, (128, 256), (24, 64), 64),\n",
        "    Inception(512, 112, (144, 288), (32, 64), 64),\n",
        "    Inception(528, 256, (160, 320), (32, 128), 128),\n",
        "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        ")\n",
        "\n",
        "# Blocco 5\n",
        "b5 = nn.Sequential(\n",
        "    Inception(832, 256, (160, 320), (32, 128), 128),\n",
        "    Inception(832, 384, (192, 384), (48, 128), 128),\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),  # Global Average Pooling\n",
        "    nn.Flatten()\n",
        ")\n",
        "\n",
        "# Rete completa\n",
        "net = nn.Sequential(b1,b2,b3,b4,b5,\n",
        "    nn.Linear(1024, 12),  # Classificazione\n",
        ")"
      ],
      "metadata": {
        "id": "bZyKhqTDlsY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training Loop di Mike\n",
        "def accuracy(predictions, targets):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    _, pred_labels = torch.max(predictions, 1)\n",
        "    total = targets.size(0)\n",
        "    correct = (pred_labels == targets).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "def training_loop(network, training_loader, validation_loader,\n",
        "                  epochs,\n",
        "                  learning_rate=1e-3, weight_decay=0.0, patience=5,\n",
        "                  device = \"cuda\",\n",
        "                  save_model_name = \"My_GoogleLeNet.pth\"):\n",
        "\n",
        "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Initialize the weights\n",
        "    def init_weights(m):\n",
        "      if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "        nn.init.xavier_uniform_(m.weight) # Xavier parameter initialization: a particular method (see textbook section)\n",
        "\n",
        "    network.apply(init_weights)\n",
        "\n",
        "    # Move the net to the GPU if is avaible\n",
        "    print(f\"Training on device {device}\\n\")\n",
        "    network.to(device)\n",
        "    # Create the optimization method\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Create the loss function\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Lists to store the training and evaluation losses/accuracy\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Start the training\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      # TRAINING LOOP\n",
        "      # Set the network in training mode\n",
        "      network.train()\n",
        "      # List to store the current epoch training loss and accuracy\n",
        "      epoch_train_loss = []\n",
        "      epoch_train_acc = []\n",
        "\n",
        "      for image, target  in tqdm(training_loader, desc = f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        #  timer.start()\n",
        "          # Reset the gradient\n",
        "          optimizer.zero_grad()\n",
        "          #MOVE each minibatch of data to the GPU (if available)\n",
        "          image, target = image.to(device), target.to(device)\n",
        "          # Compute the output\n",
        "          output = network(image)\n",
        "          # Compute the loss with Cross-Entropy for the current batch\n",
        "          loss = loss_function(output, target)\n",
        "          # Compute the gradient\n",
        "          loss.backward()\n",
        "          # Perform the update\n",
        "          optimizer.step()\n",
        "\n",
        "        #  timer.stop()\n",
        "\n",
        "          # Collect minibatch training loss to compute the average loss of the epoch\n",
        "          epoch_train_loss.append(loss.item())\n",
        "          # Compute training accuracy\n",
        "          acc = accuracy(output, target)\n",
        "          epoch_train_acc.append(acc)\n",
        "\n",
        "      # Average loss for the current epoch\n",
        "      average_loss_train = torch.mean(torch.tensor(epoch_train_loss))\n",
        "      train_losses.append(average_loss_train)\n",
        "      #Average accuracy for the current epoch\n",
        "      average_train_accuracy = torch.mean(torch.tensor(epoch_train_acc))\n",
        "      train_acc.append(average_train_accuracy)\n",
        "\n",
        "      # VALIDATION LOOP\n",
        "      # After 1 epoch of training --> full iteration over evaluation data\n",
        "      network.eval() # setting network in evaluation mode\n",
        "      epoch_val_loss = []\n",
        "      epoch_val_acc = []\n",
        "\n",
        "      with torch.no_grad(): # Set no update for the weights\n",
        "        for image, target in tqdm(validation_loader, desc = f\"Epoch {epoch+1}/{epochs}\"):\n",
        "          image, target = image.to(device), target.to(device)\n",
        "          output = network(image)\n",
        "          loss = loss_function(output, target)\n",
        "          epoch_val_loss.append(loss.item())\n",
        "          acc = accuracy(output, target)\n",
        "          epoch_val_acc.append(acc)\n",
        "\n",
        "        average_loss_val = torch.mean(torch.tensor(epoch_val_loss))\n",
        "        val_losses.append(average_loss_val)\n",
        "        average_val_accuracy = torch.mean(torch.tensor(epoch_val_acc))\n",
        "        val_acc.append(average_val_accuracy)\n",
        "\n",
        "      print(f\"EPOCH: {epoch+1} --- Train loss: {average_loss_train:7.4f} --- Eval loss: {average_loss_val:7.4f}\\n\")\n",
        "      print(f\"Train accuracy: {average_train_accuracy:7.4f} --- Eval accuracy: {average_val_accuracy:7.4f}\\n\")\n",
        "\n",
        "      # Early stopping\n",
        "      if average_loss_val < best_val_loss:\n",
        "        best_val_loss = average_loss_val\n",
        "        best_val_acc = average_val_accuracy\n",
        "        best_epoch = epoch+1\n",
        "        patience_counter = 0\n",
        "        torch.save(network.state_dict(), save_model_name) #Save the best model till the current epoch\n",
        "      else:\n",
        "        patience_counter += 1\n",
        "      if patience_counter >= patience:\n",
        "          print(f\"Early stopping triggered after {epoch+1} epochs\\n\")\n",
        "          # Load the best model at the end of training\n",
        "          network.load_state_dict(torch.load(save_model_name))\n",
        "          print(f\"BEST MODEL at EPOCH: {best_epoch+1} --- Eval loss: {best_val_loss:7.4f} --- Eval accuracy: {best_val_acc:7.4f}\")\n",
        "          break\n",
        "\n",
        "    print(\"--- Training Ended! ---\")\n",
        "    return train_losses, val_losses, train_acc, val_acc, best_epoch"
      ],
      "metadata": {
        "id": "qg34g3vglwIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
        "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
        "    if isinstance(net, torch.nn.Module):\n",
        "        net.eval()  # Set the model to evaluation mode\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # No. of correct predictions, no. of predictions\n",
        "    metric = d2l.Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            if isinstance(X, list):\n",
        "                X = [x.to(device) for x in X]\n",
        "            else:\n",
        "                X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "\n",
        "def train(net, train_iter, val_iter, test_iter, num_epochs, lr, weight_decay = 0.0, device=d2l.try_gpu()):\n",
        "    \"\"\"\n",
        "    Train a model with a GPU, similar to the professor's style.\n",
        "    Include validation and test accuracy checks without early stopping.\n",
        "    \"\"\"\n",
        "\n",
        "    # Xavier initialization\n",
        "    def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    net.apply(init_weights)\n",
        "    print('Training on', device)\n",
        "\n",
        "    net.to(device)  # MOVE the net to the GPU (if available)\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=['train loss', 'train acc', 'val acc'])\n",
        "\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "\n",
        "    progress_bar = tqdm(range(num_epochs))\n",
        "\n",
        "    for epoch in progress_bar:\n",
        "        # Sum of training loss, sum of training accuracy, no. of examples\n",
        "        metric = d2l.Accumulator(3)\n",
        "        net.train() # Set the network to training mode\n",
        "\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            optimizer.zero_grad()\n",
        "            X, y = X.to(device), y.to(device) #MOVE minibatch to GPU (if available)\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # metric: training loss sum, training correct predictions sum, no. of samples processed\n",
        "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "            timer.stop()\n",
        "\n",
        "            # Update the animator at a fraction of epochs (for visualization)\n",
        "            if (i + 1) % max(1, (num_batches // 5)) == 0 or i == num_batches - 1:\n",
        "                train_l = metric[0] / metric[2]\n",
        "                train_acc = metric[1] / metric[2]\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (train_l, train_acc, None, None))\n",
        "\n",
        "        # Evaluate on the validation and test sets after each epoch\n",
        "        val_acc = evaluate_accuracy_gpu(net, val_iter, device)\n",
        "        #test_acc = evaluate_accuracy_gpu(net, test_iter, device)\n",
        "        #animator.add(epoch + 1, (None, None, val_acc, test_acc))\n",
        "        animator.add(epoch + 1, (None, None, val_acc))\n",
        "\n",
        "\n",
        "    # Final metrics after last epoch\n",
        "    train_l = metric[0] / metric[2]\n",
        "    train_acc = metric[1] / metric[2]\n",
        "    print(f'Final Results: loss {train_l:.3f}, train acc {train_acc:.3f}, val acc {val_acc:.3f}') #, test acc {test_acc:.3f}'\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on {str(device)}')\n"
      ],
      "metadata": {
        "id": "KSn9wtkDjXr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plots delle metriche Mike"
      ],
      "metadata": {
        "id": "xwUO8GJdxRcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_metrics(train_losses, val_losses, train_acc, val_acc):\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # Plot Training and Validation Loss on the first subplot\n",
        "    axes[0].plot(train_losses, label='Training Loss', marker='o')\n",
        "    axes[0].plot(val_losses, label='Validation Loss', marker='o')\n",
        "    axes[0].set_xlabel('Epochs')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Plot Training and Validation Accuracy on the second subplot\n",
        "    axes[1].plot(train_acc, label='Training Accuracy', marker='o')\n",
        "    axes[1].plot(val_acc, label='Validation Accuracy', marker='o')\n",
        "    axes[1].set_xlabel('Epochs')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Training and Validation Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # Adjust layout and show\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_metrics(train_losses, val_losses, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "fq_OE5iSxUbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Res Net di backup**"
      ],
      "metadata": {
        "id": "tfusscpGxXGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_1x1conv=False, strides=1, dropout_prob=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=strides)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        Y = self.dropout(Y)\n",
        "        return F.relu(Y)\n",
        "\n",
        "X = torch.rand(4, 3, 64, 64)\n",
        "residual_block = Residual(3, 3, use_1x1conv=True)\n",
        "\n",
        "output = residual_block(X)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "jYo9efnlxaLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=9, dropout_prob=0.3):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self.make_layer(64, 64, 2, dropout_prob=dropout_prob)\n",
        "        self.layer2 = self.make_layer(64, 128, 2, stride=2, dropout_prob=dropout_prob)\n",
        "        self.layer3 = self.make_layer(128, 256, 2, stride=2, dropout_prob=dropout_prob)\n",
        "        self.layer4 = self.make_layer(256, 512, 2, stride=2, dropout_prob=dropout_prob)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, in_channels, out_channels, num_blocks, stride=1, dropout_prob=0.3):\n",
        "        layers = []\n",
        "        layers.append(Residual(in_channels, out_channels, strides=stride, use_1x1conv=True, dropout_prob=dropout_prob))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(Residual(out_channels, out_channels, dropout_prob=dropout_prob))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rlnqJPgOxcjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet18(num_classes=12, dropout_prob=0.3)\n",
        "\n",
        "train(net, train_loader, val_loader, test_loader, num_epochs=50, lr=0.01, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "GeNk-QQBxhF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}