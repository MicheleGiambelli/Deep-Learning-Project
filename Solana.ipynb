{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicheleGiambelli/Deep-Learning-Project/blob/Michele/Solana.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from google.colab import files\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "QvIMaRqtTCHm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = \"SOL-USD\"\n",
        "btc_ticker = \"BTC-USD\"\n",
        "\n",
        "solana_data = yf.download(ticker, start=\"2020-04-10\")\n",
        "btc_data = yf.download(btc_ticker, start=\"2020-04-10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKIIBGL8TENu",
        "outputId": "4295bbc7-24a2-445d-8f89-bb9bc77cc3a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zmXdQ5LaS7Sk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Ensure the columns are renamed every time this cell is run\n",
        "solana_data = solana_data.rename(columns={\"Adj Close\": \"SOL_Adj_Close\"})\n",
        "btc_data = btc_data.rename(columns={\"Adj Close\": \"BTC_Adj_Close\"})\n",
        "\n",
        "# Unione dei dati\n",
        "# Instead of selecting individual columns, use the renamed dataframes directly\n",
        "df = pd.concat([solana_data, btc_data], axis=1)\n",
        "df = df.dropna()\n",
        "\n",
        "# Calcolo dei rendimenti giornalieri\n",
        "df[\"SOL_Return\"] = df[\"SOL_Adj_Close\"].pct_change()\n",
        "df[\"BTC_Return\"] = df[\"BTC_Adj_Close\"].pct_change()\n",
        "\n",
        "# Funzione per calcolare Beta\n",
        "def rolling_beta(df, window):\n",
        "    cov = df[\"SOL_Return\"].rolling(window).cov(df[\"BTC_Return\"])\n",
        "    var = df[\"BTC_Return\"].rolling(window).var()\n",
        "    return cov / var\n",
        "\n",
        "# Define n before using it in rolling_beta\n",
        "n = 20  # For example, a 20-day rolling window for beta calculation\n",
        "\n",
        "# Aggiungere Beta\n",
        "df[\"Beta\"] = rolling_beta(df, n)\n",
        "df = df.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 20  # Periodo per bande di Bollinger e Beta\n",
        "k = 2  # Deviazioni standard per bande di Bollinger\n",
        "\n",
        "# Calcolo Bande di Bollinger\n",
        "df[\"SMA\"] = df[\"SOL_Adj_Close\"].rolling(window=n).mean()\n",
        "df[\"StdDev\"] = df[\"SOL_Adj_Close\"].rolling(window=n).std()\n",
        "df[\"Upper_Band\"] = df[\"SMA\"] + k * df[\"StdDev\"]\n",
        "df[\"Lower_Band\"] = df[\"SMA\"] - k * df[\"StdDev\"]\n",
        "df = df.dropna()\n"
      ],
      "metadata": {
        "id": "r-15QOT7TgMG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_sharpe_ratio(df, window, risk_free_rate):\n",
        "    rolling_mean = df[\"SOL_Return\"].rolling(window).mean()\n",
        "    rolling_std = df[\"SOL_Return\"].rolling(window).std()\n",
        "    return (rolling_mean - risk_free_rate) / rolling_std\n",
        "risk_free_rate = 0.01\n",
        "# Aggiungere Sharpe Ratio\n",
        "df[\"Sharpe_Ratio\"] = rolling_sharpe_ratio(df, n, risk_free_rate)\n",
        "df = df.dropna()\n"
      ],
      "metadata": {
        "id": "RggixdgRVy3Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df.columns[[6,7,8,9,10,11,13]], axis=1, inplace=True) # tolgo le colonne relative a BTC che non mi servono\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "si5DUOB9UDXJ",
        "outputId": "6ce222a9-9461-4fd3-c4a3-789e4c7312b0",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Price      SOL_Adj_Close     Close      High       Low      Open   Volume  \\\n",
              "Ticker           SOL-USD   SOL-USD   SOL-USD   SOL-USD   SOL-USD  SOL-USD   \n",
              "Date                                                                        \n",
              "2020-06-07      0.616578  0.616578  0.624444  0.593398  0.622443   716785   \n",
              "2020-06-08      0.668313  0.668313  0.679001  0.613310  0.615078  1440234   \n",
              "2020-06-09      0.658002  0.658002  0.668088  0.627242  0.667784   988327   \n",
              "2020-06-10      0.644867  0.644867  0.670043  0.633404  0.658038  1096203   \n",
              "2020-06-11      0.573742  0.573742  0.650535  0.570082  0.644888  1122221   \n",
              "\n",
              "Price      SOL_Return      Beta       SMA    StdDev Upper_Band Lower_Band  \\\n",
              "Ticker                                                                      \n",
              "Date                                                                        \n",
              "2020-06-07  -0.009423  0.427358  0.595371  0.027581   0.650533   0.540209   \n",
              "2020-06-08   0.083907  0.428930  0.597564  0.031483   0.660529   0.534599   \n",
              "2020-06-09  -0.015428  0.346123  0.601506  0.033900   0.669306   0.533706   \n",
              "2020-06-10  -0.019962  0.524364  0.603304  0.035240   0.673784   0.532823   \n",
              "2020-06-11  -0.110294  0.739265  0.600047  0.034787   0.669620   0.530474   \n",
              "\n",
              "Price      Sharpe_Ratio  \n",
              "Ticker                   \n",
              "Date                     \n",
              "2020-06-07    -0.201951  \n",
              "2020-06-08    -0.085599  \n",
              "2020-06-09    -0.038440  \n",
              "2020-06-10    -0.103663  \n",
              "2020-06-11    -0.229623  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79b9ee43-2fcc-441b-8342-fcccbfaf7b48\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th>Price</th>\n",
              "      <th>SOL_Adj_Close</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Volume</th>\n",
              "      <th>SOL_Return</th>\n",
              "      <th>Beta</th>\n",
              "      <th>SMA</th>\n",
              "      <th>StdDev</th>\n",
              "      <th>Upper_Band</th>\n",
              "      <th>Lower_Band</th>\n",
              "      <th>Sharpe_Ratio</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ticker</th>\n",
              "      <th>SOL-USD</th>\n",
              "      <th>SOL-USD</th>\n",
              "      <th>SOL-USD</th>\n",
              "      <th>SOL-USD</th>\n",
              "      <th>SOL-USD</th>\n",
              "      <th>SOL-USD</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-06-07</th>\n",
              "      <td>0.616578</td>\n",
              "      <td>0.616578</td>\n",
              "      <td>0.624444</td>\n",
              "      <td>0.593398</td>\n",
              "      <td>0.622443</td>\n",
              "      <td>716785</td>\n",
              "      <td>-0.009423</td>\n",
              "      <td>0.427358</td>\n",
              "      <td>0.595371</td>\n",
              "      <td>0.027581</td>\n",
              "      <td>0.650533</td>\n",
              "      <td>0.540209</td>\n",
              "      <td>-0.201951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-06-08</th>\n",
              "      <td>0.668313</td>\n",
              "      <td>0.668313</td>\n",
              "      <td>0.679001</td>\n",
              "      <td>0.613310</td>\n",
              "      <td>0.615078</td>\n",
              "      <td>1440234</td>\n",
              "      <td>0.083907</td>\n",
              "      <td>0.428930</td>\n",
              "      <td>0.597564</td>\n",
              "      <td>0.031483</td>\n",
              "      <td>0.660529</td>\n",
              "      <td>0.534599</td>\n",
              "      <td>-0.085599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-06-09</th>\n",
              "      <td>0.658002</td>\n",
              "      <td>0.658002</td>\n",
              "      <td>0.668088</td>\n",
              "      <td>0.627242</td>\n",
              "      <td>0.667784</td>\n",
              "      <td>988327</td>\n",
              "      <td>-0.015428</td>\n",
              "      <td>0.346123</td>\n",
              "      <td>0.601506</td>\n",
              "      <td>0.033900</td>\n",
              "      <td>0.669306</td>\n",
              "      <td>0.533706</td>\n",
              "      <td>-0.038440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-06-10</th>\n",
              "      <td>0.644867</td>\n",
              "      <td>0.644867</td>\n",
              "      <td>0.670043</td>\n",
              "      <td>0.633404</td>\n",
              "      <td>0.658038</td>\n",
              "      <td>1096203</td>\n",
              "      <td>-0.019962</td>\n",
              "      <td>0.524364</td>\n",
              "      <td>0.603304</td>\n",
              "      <td>0.035240</td>\n",
              "      <td>0.673784</td>\n",
              "      <td>0.532823</td>\n",
              "      <td>-0.103663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-06-11</th>\n",
              "      <td>0.573742</td>\n",
              "      <td>0.573742</td>\n",
              "      <td>0.650535</td>\n",
              "      <td>0.570082</td>\n",
              "      <td>0.644888</td>\n",
              "      <td>1122221</td>\n",
              "      <td>-0.110294</td>\n",
              "      <td>0.739265</td>\n",
              "      <td>0.600047</td>\n",
              "      <td>0.034787</td>\n",
              "      <td>0.669620</td>\n",
              "      <td>0.530474</td>\n",
              "      <td>-0.229623</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79b9ee43-2fcc-441b-8342-fcccbfaf7b48')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79b9ee43-2fcc-441b-8342-fcccbfaf7b48 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79b9ee43-2fcc-441b-8342-fcccbfaf7b48');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c0a90721-eb09-4bf6-adfd-693a88ffb9ed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c0a90721-eb09-4bf6-adfd-693a88ffb9ed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c0a90721-eb09-4bf6-adfd-693a88ffb9ed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1643,\n  \"fields\": [\n    {\n      \"column\": [\n        \"Date\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2020-06-07 00:00:00\",\n        \"max\": \"2024-12-05 00:00:00\",\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          \"2022-04-15 00:00:00\",\n          \"2023-06-12 00:00:00\",\n          \"2024-01-07 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"SOL_Adj_Close\",\n        \"SOL-USD\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66.20624177803658,\n        \"min\": 0.5706390142440796,\n        \"max\": 258.934326171875,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          101.30977630615234,\n          15.196707725524902,\n          89.2763442993164\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Close\",\n        \"SOL-USD\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66.20624177803658,\n        \"min\": 0.5706390142440796,\n        \"max\": 258.934326171875,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          101.30977630615234,\n          15.196707725524902,\n          89.2763442993164\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"High\",\n        \"SOL-USD\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 68.53382374783568,\n        \"min\": 0.5898280143737793,\n        \"max\": 263.83154296875,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          102.67264556884766,\n          15.606237411499023,\n          96.69239807128906\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Low\",\n        \"SOL-USD\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63.61689807383466,\n        \"min\": 0.5472869873046875,\n        \"max\": 253.18743896484375,\n        \"num_unique_values\": 1642,\n        \"samples\": [\n          100.10089874267578,\n          100.55369567871094,\n          22.153913497924805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Open\",\n        \"SOL-USD\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66.13123768191163,\n        \"min\": 0.5705789923667908,\n        \"max\": 258.78155517578125,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          100.70652770996094,\n          15.557231903076172,\n          93.8603515625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Volume\",\n        \"SOL-USD\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1859341080,\n        \"min\": 652020,\n        \"max\": 17068643416,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          1050639351,\n          384209938,\n          2288061692\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"SOL_Return\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06775413887943181,\n        \"min\": -0.4228090368113009,\n        \"max\": 0.4728278503308343,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          0.005977436468998842,\n          -0.02327099371794239,\n          -0.0488119160609648\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Beta\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6902916498514924,\n        \"min\": -1.7336492413990903,\n        \"max\": 4.9057313734445644,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          1.5857674598452685,\n          1.1689148500883826,\n          1.6401644328666407\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"SMA\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 64.49086878790001,\n        \"min\": 0.5953709006309509,\n        \"max\": 238.7207275390625,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          115.3580883026123,\n          19.4141565322876,\n          101.13188323974609\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"StdDev\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.865292303631765,\n        \"min\": 0.027580882484290914,\n        \"max\": 41.9771533393623,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          12.05400324268723,\n          2.004324620500641,\n          10.997920128351414\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Upper_Band\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75.82869596428269,\n        \"min\": 0.6505326655995327,\n        \"max\": 271.72830817528927,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          139.46609478798678,\n          23.422805773288882,\n          123.12772349644891\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Lower_Band\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54.2697990571838,\n        \"min\": 0.4973684141239937,\n        \"max\": 217.72299564350308,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          91.25008181723784,\n          15.405507291286316,\n          79.13604298304327\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"Sharpe_Ratio\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27807218440308584,\n        \"min\": -0.7930020271025044,\n        \"max\": 0.6808430420358316,\n        \"num_unique_values\": 1643,\n        \"samples\": [\n          -0.1499705644083428,\n          -0.5146892583976307,\n          0.020467398874488667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h1>Pytorch Dataset</h1>**"
      ],
      "metadata": {
        "id": "C4MRR0d1i_B2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of the Code below\n",
        "\n",
        "This code demonstrates how to preprocess a dataset stored in a Pandas DataFrame and transform it into a format suitable for use with PyTorch, while ensuring the data can be efficiently loaded in batches for training or inference.\n",
        "\n",
        "1. **Feature and Target Definition**:\n",
        "   - The column `Close` is chosen as the target variable (`y`), representing the Solana price.\n",
        "   - The remaining columns are used as the feature set (`X`).\n",
        "\n",
        "2. **Feature Normalization**:\n",
        "   - The features are scaled using `StandardScaler` from `sklearn` to ensure they have zero mean and unit variance. This helps improve the stability and convergence of training neural networks.\n",
        "\n",
        "3. **Conversion to PyTorch Tensors**:\n",
        "   - The normalized features (`X`) are converted into a PyTorch tensor of type `torch.float32`.\n",
        "   - The target (`y`) is similarly converted into a tensor, reshaped to have a shape of `(-1, 1)` to align with PyTorch's supervised learning expectations.\n",
        "\n",
        "4. **Custom DataLoader Function**:\n",
        "   - A function, `load_array`, is defined to create a PyTorch `DataLoader`. This function:\n",
        "     - Accepts `data_arrays` (a tuple of feature and target tensors).\n",
        "     - Uses unpacking (`*data_arrays`) to pass the tensors dynamically to `TensorDataset`.\n",
        "     - Returns a `DataLoader` with a specified batch size.\n",
        "     - Includes the `is_train` parameter to control whether data shuffling is enabled. Here, it defaults to `False` to maintain the historical order of the data.\n",
        "\n",
        "5. **Batch Loading**:\n",
        "   - The tensors for features (`X_tensor`) and target (`y_tensor`) are packed into a tuple `data_arrays`.\n",
        "   - A `DataLoader` is created with a batch size of 32, ensuring that the data is processed in manageable chunks.\n",
        "\n",
        "This code is well-suited for historical data processing (e.g., time-series or financial data) where the order of data is important, thanks to the use of `shuffle=False`. The preprocessing ensures the dataset is ready for efficient model training or evaluation in PyTorch."
      ],
      "metadata": {
        "id": "rbwnon3cuNL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume your DataFrame pandas is named `df`\n",
        "# Define 'Close' as the target and the rest as features\n",
        "X = df.drop(columns=['Close'], level = 0).values  # Features\n",
        "y = df['Close'].values  # Target\n",
        "\n",
        "# Normalize the features, referring to the same numerical scale\n",
        "scaler = StandardScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "y_normalized = scaler.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_normalized, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_normalized, dtype=torch.float32)\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 5\n",
        "X_sequences = []\n",
        "y_sequences = []\n",
        "\n",
        "for i in range(len(X_normalized) - sequence_length):\n",
        "    X_seq = X_normalized[i:i + sequence_length]\n",
        "    y_seq = y[i + sequence_length]\n",
        "    X_sequences.append(X_seq)\n",
        "    y_sequences.append(y_seq)\n",
        "\n",
        "X_sequences = np.array(X_sequences)\n",
        "y_sequences = np.array(y_sequences)\n",
        "\n",
        "X_tensor = torch.tensor(X_sequences, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_sequences, dtype=torch.float32)\n",
        "\n",
        "# Suddivisione in training e validation set\n",
        "# Suddivisione temporale del dataset\n",
        "n = len(X_tensor)\n",
        "\n",
        "# Percentuali di suddivisione\n",
        "train_split = int(n * 0.7)  # Prime 70% osservazioni per il training\n",
        "val_split = int(n * 0.9)    # Successive 20% osservazioni per la validation\n",
        "test_split = n              # Ultime 10% osservazioni per il test\n",
        "\n",
        "train_dataset = (X_tensor[:train_split], y_tensor[:train_split])\n",
        "val_dataset = (X_tensor[train_split:val_split], y_tensor[train_split:val_split])\n",
        "test_dataset = (X_tensor[val_split:test_split], y_tensor[val_split:test_split])\n",
        "\n",
        "# Define a function for loading data with unpacking\n",
        "def load_array(data_arrays, batch_size, is_train=False): # False because we want to read the data historically\n",
        "    \"\"\"Construct a PyTorch DataLoader.\"\"\"\n",
        "    dataset = TensorDataset(*data_arrays)  # Unpacking the tensors\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
        "\n",
        "# Create the DataLoader\n",
        "data_arrays = (X_tensor, y_tensor)  # Pack tensors into a tuple\n",
        "batch_size = 32\n",
        "train_loader = load_array(train_dataset, batch_size)\n",
        "valid_loader = load_array(val_dataset, batch_size)\n",
        "test_loader = load_array(test_dataset, batch_size)\n"
      ],
      "metadata": {
        "id": "z3GjpGRSi-Zx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_X, batch_y = next(iter(train_loader))\n",
        "print(batch_X.shape)  # Expected: torch.Size([32, 10, 12])\n",
        "print(batch_y.shape)"
      ],
      "metadata": {
        "id": "Q-WDl08Gp6Zc",
        "outputId": "acd4ca1b-79f2-4968-b6d3-9dd7eb1b1a65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 5, 12])\n",
            "torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "1NSIQI8PFA6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stato nascosto iniziale e stato della cella\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Passaggio attraverso la LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Passaggio attraverso il layer fully connected\n",
        "        out = self.fc(out[:, -1, :])  # Usare solo l'ultimo stato nascosto\n",
        "        return out"
      ],
      "metadata": {
        "id": "ypm-AA2vFARy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iperparametri\n",
        "input_size = 12\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "\n",
        "# Inizializzazione del modello, della loss function e dell'optimizer\n",
        "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "LRmOtCHZPyMZ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Addestramento del modello\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "# Early stopping\n",
        "best_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False\n",
        "\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    if early_stop:\n",
        "        print(f\"Early stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_mae = 0.0\n",
        "    y_preds, y_real = [], []\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass e aggiornamento dei pesi\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calcolo del MAE\n",
        "        y_preds.extend(outputs.detach().cpu().numpy())\n",
        "        y_real.extend(batch_y.cpu().numpy())\n",
        "        # predictions = outputs.detach().cpu().numpy()  # Denormalizza le predizioni\n",
        "        # actuals = scaler.inverse_transform(batch_y.numpy())  # Denormalizza i target\n",
        "        # train_mae += mean_absolute_error(actuals, predictions)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_mae = mean_absolute_error(y_real, y_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    # val_mae = 0.0\n",
        "    y_preds, y_real = [], []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_X, val_y in valid_loader:\n",
        "            val_outputs = model(val_X)\n",
        "            loss = criterion(val_outputs, val_y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calcolo del MAE per il validation set\n",
        "            y_preds.extend(outputs.detach().cpu().numpy())\n",
        "            y_real.extend(batch_y.cpu().numpy())\n",
        "\n",
        "            # val_predictions = scaler.inverse_transform(val_outputs.numpy())\n",
        "            # val_actuals = scaler.inverse_transform(val_y.numpy())\n",
        "            # val_mae += mean_absolute_error(val_actuals, val_predictions)\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    # val_mae /= len(valid_loader)\n",
        "    val_mae = mean_absolute_error(y_real, y_preds)\n",
        "\n",
        "\n",
        "    # Stampa della loss e del MAE ogni 10 epoche\n",
        "    # if (epoch + 1) %2 == 0:\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Early stopping check\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      epochs_no_improve = 0\n",
        "      # Salva i pesi del miglior modello\n",
        "      best_epoch = epoch+1\n",
        "      best_model_weights = model.state_dict()\n",
        "    elif val_loss > best_loss+0.001:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= 5:\n",
        "        early_stop = True\n",
        "\n",
        "if best_model_weights is not None:\n",
        "   model.load_state_dict(best_model_weights)\n",
        "   print(f\"Weights of best epoch {best_epoch} uploaded {best_loss}.\")"
      ],
      "metadata": {
        "id": "dCE5sKUHRwcq",
        "outputId": "4b658e81-4ac9-45ed-8d87-93283ff254ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Train Loss: 5452.2496, Train MAE: 46.0098, Val Loss: 11390.7333, Val MAE: 16.5236\n",
            "Epoch [2/200], Train Loss: 4737.8772, Train MAE: 40.9565, Val Loss: 10199.9257, Val MAE: 12.1298\n",
            "Epoch [3/200], Train Loss: 4438.6283, Train MAE: 39.2018, Val Loss: 9690.0341, Val MAE: 9.0332\n",
            "Epoch [4/200], Train Loss: 4249.8868, Train MAE: 38.1356, Val Loss: 9271.3195, Val MAE: 6.3967\n",
            "Epoch [5/200], Train Loss: 4099.1237, Train MAE: 37.3590, Val Loss: 8910.1502, Val MAE: 4.0524\n",
            "Epoch [6/200], Train Loss: 3974.1109, Train MAE: 36.9032, Val Loss: 8591.5486, Val MAE: 2.3154\n",
            "Epoch [7/200], Train Loss: 3868.7219, Train MAE: 36.6732, Val Loss: 8307.3341, Val MAE: 1.5473\n",
            "Epoch [8/200], Train Loss: 3779.0808, Train MAE: 36.5920, Val Loss: 8052.0860, Val MAE: 2.0757\n",
            "Epoch [9/200], Train Loss: 3702.0532, Train MAE: 36.5558, Val Loss: 7821.4944, Val MAE: 3.4192\n",
            "Epoch [10/200], Train Loss: 3638.7742, Train MAE: 36.7916, Val Loss: 7607.6673, Val MAE: 4.9015\n",
            "Epoch [11/200], Train Loss: 3549.2151, Train MAE: 35.3280, Val Loss: 7389.5022, Val MAE: 3.6062\n",
            "Epoch [12/200], Train Loss: 3397.0116, Train MAE: 33.3964, Val Loss: 7160.6849, Val MAE: 8.4058\n",
            "Epoch [13/200], Train Loss: 3464.4922, Train MAE: 36.6170, Val Loss: 6993.2893, Val MAE: 8.1294\n",
            "Epoch [14/200], Train Loss: 3362.0961, Train MAE: 35.8030, Val Loss: 6802.3867, Val MAE: 5.6173\n",
            "Epoch [15/200], Train Loss: 3225.8492, Train MAE: 33.2062, Val Loss: 6591.4556, Val MAE: 1.8460\n",
            "Epoch [16/200], Train Loss: 3026.7190, Train MAE: 29.6386, Val Loss: 6363.3146, Val MAE: 7.0257\n",
            "Epoch [17/200], Train Loss: 2893.7001, Train MAE: 26.3350, Val Loss: 6138.2200, Val MAE: 2.9998\n",
            "Epoch [18/200], Train Loss: 2806.8873, Train MAE: 25.7881, Val Loss: 5920.9540, Val MAE: 1.6084\n",
            "Epoch [19/200], Train Loss: 2718.8790, Train MAE: 24.4332, Val Loss: 5712.7083, Val MAE: 1.4963\n",
            "Epoch [20/200], Train Loss: 2636.0860, Train MAE: 23.5645, Val Loss: 5509.9421, Val MAE: 1.5062\n",
            "Epoch [21/200], Train Loss: 2557.8100, Train MAE: 23.1179, Val Loss: 5313.6196, Val MAE: 1.3387\n",
            "Epoch [22/200], Train Loss: 2482.6847, Train MAE: 22.7892, Val Loss: 5124.5971, Val MAE: 1.2092\n",
            "Epoch [23/200], Train Loss: 2410.8792, Train MAE: 22.5653, Val Loss: 4943.1177, Val MAE: 1.3043\n",
            "Epoch [24/200], Train Loss: 2343.7870, Train MAE: 22.6121, Val Loss: 4768.9116, Val MAE: 1.6362\n",
            "Epoch [25/200], Train Loss: 2280.7250, Train MAE: 22.6321, Val Loss: 4598.2421, Val MAE: 1.7732\n",
            "Epoch [26/200], Train Loss: 2215.3058, Train MAE: 22.2346, Val Loss: 4432.2614, Val MAE: 1.9615\n",
            "Epoch [27/200], Train Loss: 2147.4625, Train MAE: 21.5271, Val Loss: 4273.4148, Val MAE: 2.1257\n",
            "Epoch [28/200], Train Loss: 2083.4272, Train MAE: 21.0012, Val Loss: 4120.6429, Val MAE: 2.1852\n",
            "Epoch [29/200], Train Loss: 2023.9051, Train MAE: 20.6627, Val Loss: 3973.6247, Val MAE: 2.1941\n",
            "Epoch [30/200], Train Loss: 1963.8858, Train MAE: 20.0986, Val Loss: 3829.8405, Val MAE: 2.0862\n",
            "Epoch [31/200], Train Loss: 1905.6999, Train MAE: 19.5679, Val Loss: 3690.3019, Val MAE: 1.7930\n",
            "Epoch [32/200], Train Loss: 1850.2168, Train MAE: 19.1149, Val Loss: 3554.9675, Val MAE: 1.8018\n",
            "Epoch [33/200], Train Loss: 1796.2438, Train MAE: 18.6622, Val Loss: 3425.0871, Val MAE: 1.3824\n",
            "Epoch [34/200], Train Loss: 1744.6297, Train MAE: 18.2790, Val Loss: 3299.7277, Val MAE: 1.4273\n",
            "Epoch [35/200], Train Loss: 1695.7058, Train MAE: 18.0510, Val Loss: 3183.9314, Val MAE: 1.4150\n",
            "Epoch [36/200], Train Loss: 1649.5369, Train MAE: 17.9626, Val Loss: 3068.5535, Val MAE: 1.4276\n",
            "Epoch [37/200], Train Loss: 1605.3454, Train MAE: 17.8437, Val Loss: 2955.3722, Val MAE: 1.1935\n",
            "Epoch [38/200], Train Loss: 1561.2635, Train MAE: 17.5376, Val Loss: 2848.4933, Val MAE: 2.4444\n",
            "Epoch [39/200], Train Loss: 1520.2922, Train MAE: 17.4791, Val Loss: 2749.1066, Val MAE: 3.2445\n",
            "Epoch [40/200], Train Loss: 1493.9922, Train MAE: 18.4748, Val Loss: 2644.2870, Val MAE: 2.7727\n",
            "Epoch [41/200], Train Loss: 1452.7556, Train MAE: 17.8393, Val Loss: 2553.1990, Val MAE: 1.6515\n",
            "Epoch [42/200], Train Loss: 1394.5447, Train MAE: 16.1701, Val Loss: 2445.7360, Val MAE: 1.8929\n",
            "Epoch [43/200], Train Loss: 1353.3810, Train MAE: 15.5850, Val Loss: 2355.3512, Val MAE: 2.3821\n",
            "Epoch [44/200], Train Loss: 1319.4552, Train MAE: 15.6618, Val Loss: 2269.3725, Val MAE: 2.7301\n",
            "Epoch [45/200], Train Loss: 1283.0888, Train MAE: 15.2885, Val Loss: 2188.2735, Val MAE: 2.9589\n",
            "Epoch [46/200], Train Loss: 1248.5142, Train MAE: 14.9573, Val Loss: 2106.9645, Val MAE: 2.5921\n",
            "Epoch [47/200], Train Loss: 1215.0306, Train MAE: 14.6758, Val Loss: 2057.3384, Val MAE: 2.7519\n",
            "Epoch [48/200], Train Loss: 1187.0258, Train MAE: 14.5894, Val Loss: 1954.7955, Val MAE: 1.7542\n",
            "Epoch [49/200], Train Loss: 1151.8226, Train MAE: 14.0790, Val Loss: 1882.6876, Val MAE: 2.5534\n",
            "Epoch [50/200], Train Loss: 1122.2998, Train MAE: 13.8107, Val Loss: 1813.7667, Val MAE: 1.4297\n",
            "Epoch [51/200], Train Loss: 1095.0496, Train MAE: 13.7088, Val Loss: 1816.6099, Val MAE: 1.8881\n",
            "Epoch [52/200], Train Loss: 1085.9294, Train MAE: 14.3102, Val Loss: 1689.3172, Val MAE: 1.2453\n",
            "Epoch [53/200], Train Loss: 1045.9153, Train MAE: 13.8385, Val Loss: 1625.2718, Val MAE: 1.7306\n",
            "Epoch [54/200], Train Loss: 1022.7192, Train MAE: 13.9759, Val Loss: 1599.0354, Val MAE: 1.4546\n",
            "Epoch [55/200], Train Loss: 991.5947, Train MAE: 13.1758, Val Loss: 1506.4074, Val MAE: 3.5801\n",
            "Epoch [56/200], Train Loss: 965.3478, Train MAE: 12.7552, Val Loss: 1454.7253, Val MAE: 1.2273\n",
            "Epoch [57/200], Train Loss: 943.8060, Train MAE: 12.7994, Val Loss: 1442.6922, Val MAE: 1.5105\n",
            "Epoch [58/200], Train Loss: 922.1677, Train MAE: 12.4749, Val Loss: 1345.9879, Val MAE: 3.2021\n",
            "Epoch [59/200], Train Loss: 902.2474, Train MAE: 12.8691, Val Loss: 1307.7287, Val MAE: 1.3823\n",
            "Epoch [60/200], Train Loss: 887.6934, Train MAE: 12.9353, Val Loss: 1256.5399, Val MAE: 1.2359\n",
            "Epoch [61/200], Train Loss: 851.5414, Train MAE: 11.8951, Val Loss: 1221.9829, Val MAE: 1.5900\n",
            "Epoch [62/200], Train Loss: 828.5740, Train MAE: 11.5473, Val Loss: 1169.9336, Val MAE: 2.8503\n",
            "Epoch [63/200], Train Loss: 815.3351, Train MAE: 11.9637, Val Loss: 1270.4776, Val MAE: 1.4465\n",
            "Epoch [64/200], Train Loss: 812.2997, Train MAE: 12.3634, Val Loss: 1081.1645, Val MAE: 4.6835\n",
            "Epoch [65/200], Train Loss: 782.9574, Train MAE: 12.1878, Val Loss: 1046.8480, Val MAE: 1.2617\n",
            "Epoch [66/200], Train Loss: 765.4712, Train MAE: 12.0840, Val Loss: 1404.3836, Val MAE: 1.2944\n",
            "Epoch [67/200], Train Loss: 790.9758, Train MAE: 12.2505, Val Loss: 989.0525, Val MAE: 2.5596\n",
            "Epoch [68/200], Train Loss: 747.9461, Train MAE: 12.6039, Val Loss: 1004.5685, Val MAE: 1.1812\n",
            "Epoch [69/200], Train Loss: 729.3682, Train MAE: 11.5582, Val Loss: 1006.6774, Val MAE: 3.1520\n",
            "Epoch [70/200], Train Loss: 749.6095, Train MAE: 13.3936, Val Loss: 883.3663, Val MAE: 2.2378\n",
            "Epoch [71/200], Train Loss: 692.6206, Train MAE: 12.0868, Val Loss: 1266.3861, Val MAE: 2.7803\n",
            "Epoch [72/200], Train Loss: 808.9586, Train MAE: 13.0584, Val Loss: 937.8705, Val MAE: 3.9598\n",
            "Epoch [73/200], Train Loss: 718.1260, Train MAE: 12.1856, Val Loss: 828.9943, Val MAE: 1.8810\n",
            "Epoch [74/200], Train Loss: 646.3403, Train MAE: 10.6119, Val Loss: 753.2518, Val MAE: 2.7686\n",
            "Epoch [75/200], Train Loss: 611.6599, Train MAE: 10.2733, Val Loss: 728.2001, Val MAE: 3.2028\n",
            "Epoch [76/200], Train Loss: 591.7231, Train MAE: 9.9679, Val Loss: 684.5482, Val MAE: 3.0572\n",
            "Epoch [77/200], Train Loss: 577.2997, Train MAE: 9.8503, Val Loss: 733.5131, Val MAE: 2.3447\n",
            "Epoch [78/200], Train Loss: 564.5949, Train MAE: 9.7579, Val Loss: 651.2803, Val MAE: 3.4096\n",
            "Epoch [79/200], Train Loss: 557.1590, Train MAE: 9.9634, Val Loss: 826.5947, Val MAE: 1.8875\n",
            "Epoch [80/200], Train Loss: 552.6259, Train MAE: 10.0012, Val Loss: 633.7968, Val MAE: 3.6393\n",
            "Epoch [81/200], Train Loss: 550.7381, Train MAE: 10.4264, Val Loss: 612.4944, Val MAE: 1.1127\n",
            "Epoch [82/200], Train Loss: 522.4585, Train MAE: 9.9952, Val Loss: 565.6383, Val MAE: 3.0866\n",
            "Epoch [83/200], Train Loss: 513.7918, Train MAE: 10.1426, Val Loss: 582.0784, Val MAE: 1.4227\n",
            "Epoch [84/200], Train Loss: 488.3255, Train MAE: 8.8517, Val Loss: 509.5864, Val MAE: 1.5601\n",
            "Epoch [85/200], Train Loss: 481.1436, Train MAE: 9.2544, Val Loss: 509.2571, Val MAE: 1.5968\n",
            "Epoch [86/200], Train Loss: 462.4667, Train MAE: 8.5345, Val Loss: 464.5378, Val MAE: 2.0308\n",
            "Epoch [87/200], Train Loss: 455.9080, Train MAE: 9.0711, Val Loss: 460.1365, Val MAE: 1.6411\n",
            "Epoch [88/200], Train Loss: 438.0623, Train MAE: 8.2910, Val Loss: 413.3933, Val MAE: 1.4804\n",
            "Epoch [89/200], Train Loss: 427.4207, Train MAE: 8.3267, Val Loss: 433.7297, Val MAE: 1.1900\n",
            "Epoch [90/200], Train Loss: 414.8899, Train MAE: 7.9650, Val Loss: 381.9125, Val MAE: 1.9351\n",
            "Epoch [91/200], Train Loss: 406.9698, Train MAE: 8.1691, Val Loss: 403.8341, Val MAE: 1.2545\n",
            "Epoch [92/200], Train Loss: 393.4144, Train MAE: 7.6253, Val Loss: 351.3157, Val MAE: 1.3264\n",
            "Epoch [93/200], Train Loss: 387.2757, Train MAE: 7.9002, Val Loss: 402.2635, Val MAE: 1.3487\n",
            "Epoch [94/200], Train Loss: 377.2865, Train MAE: 7.7692, Val Loss: 333.6882, Val MAE: 2.5703\n",
            "Epoch [95/200], Train Loss: 373.8366, Train MAE: 8.1015, Val Loss: 416.6995, Val MAE: 1.1802\n",
            "Epoch [96/200], Train Loss: 363.5738, Train MAE: 7.7989, Val Loss: 334.4096, Val MAE: 2.4030\n",
            "Epoch [97/200], Train Loss: 364.4361, Train MAE: 8.3145, Val Loss: 442.0036, Val MAE: 1.0575\n",
            "Epoch [98/200], Train Loss: 352.9258, Train MAE: 7.7533, Val Loss: 362.1315, Val MAE: 2.4401\n",
            "Epoch [99/200], Train Loss: 363.8519, Train MAE: 8.6654, Val Loss: 486.0118, Val MAE: 1.2676\n",
            "Early stopping at epoch 99\n",
            "Weights of best epoch 93 uploaded 333.688194101507.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Valutazione finale\n",
        "X_test, y_test = next(iter(train_loader))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test).numpy()\n",
        "    predictions = scaler.inverse_transform(predictions)  # Denormalizza\n",
        "    actuals = scaler.inverse_transform(y_test.numpy())\n",
        "\n",
        "# Output delle performance\n",
        "print(\"Predizioni vs Valori Reali\")\n",
        "print(np.concatenate((predictions[:10], actuals[:10]), axis=1))  # Mostra i primi 10 risultati"
      ],
      "metadata": {
        "id": "v0SN4UZuQVFw",
        "outputId": "b71e519f-fa52-4b9c-cb6e-d2fb501a7886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predizioni vs Valori Reali\n",
            "[[193.6508   108.08182 ]\n",
            " [219.04265  109.079506]\n",
            " [241.48936  107.632286]\n",
            " [222.2844   106.54432 ]\n",
            " [212.59007  107.532936]\n",
            " [220.9826   107.267265]\n",
            " [220.54163  109.8746  ]\n",
            " [243.81787  114.069214]\n",
            " [278.4994   111.37273 ]\n",
            " [230.44618  113.329056]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il formato del tensore torch.Size([32, 12]) che ottieni con next(iter(data_loader))[0].shape  parzialmente corretto per creare una LSTM, ma manca una dimensione fondamentale: la dimensione di sequenza temporale.\n",
        "\n",
        "Per una LSTM, il tensore di input dovrebbe avere la seguente forma:\n",
        "\n",
        "(\n",
        "batch_size\n",
        ",\n",
        "sequence_length\n",
        ",\n",
        "input_size\n",
        ")\n",
        "(batch_size,sequence_length,input_size)"
      ],
      "metadata": {
        "id": "uzqGgTCYLKCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spesso la scelta di sequence_length viene fatta sperimentalmente:\n",
        "\n",
        "Prova diversi valori (ad esempio, 1, 5, 7) e confronta le prestazioni del modello (ad esempio, tramite metriche come MSE o MAE).\n",
        "Usa la validazione incrociata per capire quale valore produce le previsioni pi accurate.\n"
      ],
      "metadata": {
        "id": "hD0Y4OB3M3jx"
      }
    }
  ]
}